{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccbfc0d-3c8d-4360-bace-1b5c9ddb1d9a",
   "metadata": {},
   "source": [
    "# Query 5\n",
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8d82bb-1221-4165-b899-1602aca8dfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '8', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1805</td><td>application_1765289937462_1789</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1789/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1789_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1807</td><td>application_1765289937462_1791</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1791/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1791_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1811</td><td>application_1765289937462_1795</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1795/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1795_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1813</td><td>application_1765289937462_1797</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1797/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1797_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1815</td><td>application_1765289937462_1799</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1799/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1799_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"8\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9fa21a-e75a-4f29-9a65-ebd224514be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1816</td><td>application_1765289937462_1800</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1800/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-213.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1800_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b9daca42c3458ba39cb6a303af9e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e0bba59c734c7fb64f99c91d617367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Correlation ---\n",
      "+--------------------+\n",
      "|         Correlation|\n",
      "+--------------------+\n",
      "|-0.18052205098073226|\n",
      "+--------------------+\n",
      "\n",
      "Correlation Top 10:\n",
      "+-------------------+\n",
      "| Correlation_Top_10|\n",
      "+-------------------+\n",
      "|-0.5235595963528522|\n",
      "+-------------------+\n",
      "\n",
      "Correlation Bottom 10:\n",
      "+---------------------+\n",
      "|Correlation_Bottom_10|\n",
      "+---------------------+\n",
      "|  0.23946733929772732|\n",
      "+---------------------+\n",
      "\n",
      "\n",
      "Execution time: 96.10150384902954 seconds\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(6) Project [COMM#50, avg income/person#323, (cast(crime_count#356L as double) / (population#317 * 2.0)) AS avg crime/person#397]\n",
      "   +- *(6) BroadcastHashJoin [COMM#50], [COMM#371], Inner, BuildLeft, false\n",
      "      :- BroadcastQueryStage 3\n",
      "      :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=527]\n",
      "      :     +- *(5) Project [COMM#50, population#317, (sum_income#319 / population#317) AS avg income/person#323]\n",
      "      :        +- *(5) HashAggregate(keys=[COMM#50], functions=[sum(population_2020#190), sum(income_per_block#301)], schema specialized)\n",
      "      :           +- AQEShuffleRead coalesced\n",
      "      :              +- ShuffleQueryStage 2\n",
      "      :                 +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=470]\n",
      "      :                    +- *(4) HashAggregate(keys=[COMM#50], functions=[partial_sum(population_2020#190), partial_sum(income_per_block#301)], schema specialized)\n",
      "      :                       +- *(4) Project [population_2020#190, COMM#50, (income_2021#206 * cast(housing_2020#193L as double)) AS income_per_block#301]\n",
      "      :                          +- *(4) BroadcastHashJoin [zip_code_key#191], [zip_code_key#205], Inner, BuildRight, false\n",
      "      :                             :- *(4) Project [cast(features#33.properties.POP20 as double) AS population_2020#190, features#33.properties.ZCTA20 AS zip_code_key#191, features#33.properties.HOUSING20 AS housing_2020#193L, features#33.properties.COMM AS COMM#50]\n",
      "      :                             :  +- *(4) Filter ((isnotnull(features#33.properties.POP20) AND (isnotnull(features#33.properties.ZCTA20) AND (cast(features#33.properties.POP20 as double) > 0.0))) AND isnotnull(features#33.properties.COMM))\n",
      "      :                             :     +- *(4) Generate explode(features#25), false, [features#33]\n",
      "      :                             :        +- *(4) Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :                             :           +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      :                             +- BroadcastQueryStage 0\n",
      "      :                                +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=328]\n",
      "      :                                   +- *(1) Project [Zip Code#199 AS zip_code_key#205, cast(regexp_replace(Estimated Median Income#201, [$,], , 1) as double) AS income_2021#206]\n",
      "      :                                      +- *(1) Filter isnotnull(Zip Code#199)\n",
      "      :                                         +- FileScan csv [Zip Code#199,Estimated Median Income#201] Batched: false, DataFilters: [isnotnull(Zip Code#199)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      +- *(6) HashAggregate(keys=[COMM#371], functions=[count(DR_NO#210)], schema specialized)\n",
      "         +- AQEShuffleRead coalesced\n",
      "            +- ShuffleQueryStage 1\n",
      "               +- Exchange hashpartitioning(COMM#371, 1000), ENSURE_REQUIREMENTS, [plan_id=358]\n",
      "                  +- *(3) HashAggregate(keys=[COMM#371], functions=[partial_count(DR_NO#210)], schema specialized)\n",
      "                     +- *(3) Project [DR_NO#210, COMM#371]\n",
      "                        +- RangeJoin crime_geom#271: geometry, census_geom#192: geometry, WITHIN\n",
      "                           :- Project [DR_NO#210,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#271]\n",
      "                           :  +- Filter (((((isnotnull(LAT#236) AND isnotnull(LON#237)) AND (NOT (LAT#236 = 0.0) OR NOT (LON#237 = 0.0))) AND isnotnull(year(cast(gettimestamp(Date Rptd#211, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)))) AND ((year(cast(gettimestamp(Date Rptd#211, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) = 2020) OR (year(cast(gettimestamp(Date Rptd#211, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) = 2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                           :     +- FileScan csv [DR_NO#210,Date Rptd#211,LAT#236,LON#237] Batched: false, DataFilters: [isnotnull(LAT#236), isnotnull(LON#237), (NOT (LAT#236 = 0.0) OR NOT (LON#237 = 0.0)), isnotnull(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,LAT:double,LON:double>\n",
      "                           +- *(2) Project [features#33.geometry AS census_geom#192, features#33.properties.COMM AS COMM#371]\n",
      "                              +- *(2) Filter ((((isnotnull(features#33.properties.POP20) AND isnotnull(features#33.properties.ZCTA20)) AND (cast(features#33.properties.POP20 as double) > 0.0)) AND isnotnull(features#33.geometry)) AND isnotnull(features#33.properties.COMM))\n",
      "                                 +- *(2) Generate explode(features#360), false, [features#33]\n",
      "                                    +- *(2) Filter ((size(features#360, true) > 0) AND isnotnull(features#360))\n",
      "                                       +- FileScan geojson [features#360] Batched: false, DataFilters: [(size(features#360, true) > 0), isnotnull(features#360)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "+- == Initial Plan ==\n",
      "   Project [COMM#50, avg income/person#323, (cast(crime_count#356L as double) / (population#317 * 2.0)) AS avg crime/person#397]\n",
      "   +- SortMergeJoin [COMM#50], [COMM#371], Inner\n",
      "      :- Sort [COMM#50 ASC NULLS FIRST], false, 0\n",
      "      :  +- Project [COMM#50, population#317, (sum_income#319 / population#317) AS avg income/person#323]\n",
      "      :     +- HashAggregate(keys=[COMM#50], functions=[sum(population_2020#190), sum(income_per_block#301)], schema specialized)\n",
      "      :        +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=199]\n",
      "      :           +- HashAggregate(keys=[COMM#50], functions=[partial_sum(population_2020#190), partial_sum(income_per_block#301)], schema specialized)\n",
      "      :              +- Project [population_2020#190, COMM#50, (income_2021#206 * cast(housing_2020#193L as double)) AS income_per_block#301]\n",
      "      :                 +- BroadcastHashJoin [zip_code_key#191], [zip_code_key#205], Inner, BuildRight, false\n",
      "      :                    :- Project [cast(features#33.properties.POP20 as double) AS population_2020#190, features#33.properties.ZCTA20 AS zip_code_key#191, features#33.properties.HOUSING20 AS housing_2020#193L, features#33.properties.COMM AS COMM#50]\n",
      "      :                    :  +- Filter ((isnotnull(features#33.properties.POP20) AND (isnotnull(features#33.properties.ZCTA20) AND (cast(features#33.properties.POP20 as double) > 0.0))) AND isnotnull(features#33.properties.COMM))\n",
      "      :                    :     +- Generate explode(features#25), false, [features#33]\n",
      "      :                    :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :                    :           +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      :                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=194]\n",
      "      :                       +- Project [Zip Code#199 AS zip_code_key#205, cast(regexp_replace(Estimated Median Income#201, [$,], , 1) as double) AS income_2021#206]\n",
      "      :                          +- Filter isnotnull(Zip Code#199)\n",
      "      :                             +- FileScan csv [Zip Code#199,Estimated Median Income#201] Batched: false, DataFilters: [isnotnull(Zip Code#199)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      +- Sort [COMM#371 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#371], functions=[count(DR_NO#210)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#371, 1000), ENSURE_REQUIREMENTS, [plan_id=267]\n",
      "               +- HashAggregate(keys=[COMM#371], functions=[partial_count(DR_NO#210)], schema specialized)\n",
      "                  +- Project [DR_NO#210, COMM#371]\n",
      "                     +- RangeJoin crime_geom#271: geometry, census_geom#192: geometry, WITHIN\n",
      "                        :- Project [DR_NO#210,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#271]\n",
      "                        :  +- Filter (((((isnotnull(LAT#236) AND isnotnull(LON#237)) AND (NOT (LAT#236 = 0.0) OR NOT (LON#237 = 0.0))) AND isnotnull(year(cast(gettimestamp(Date Rptd#211, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)))) AND ((year(cast(gettimestamp(Date Rptd#211, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) = 2020) OR (year(cast(gettimestamp(Date Rptd#211, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) = 2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                        :     +- FileScan csv [DR_NO#210,Date Rptd#211,LAT#236,LON#237] Batched: false, DataFilters: [isnotnull(LAT#236), isnotnull(LON#237), (NOT (LAT#236 = 0.0) OR NOT (LON#237 = 0.0)), isnotnull(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,LAT:double,LON:double>\n",
      "                        +- Project [features#33.geometry AS census_geom#192, features#33.properties.COMM AS COMM#371]\n",
      "                           +- Filter ((((isnotnull(features#33.properties.POP20) AND isnotnull(features#33.properties.ZCTA20)) AND (cast(features#33.properties.POP20 as double) > 0.0)) AND isnotnull(features#33.geometry)) AND isnotnull(features#33.properties.COMM))\n",
      "                              +- Generate explode(features#360), false, [features#33]\n",
      "                                 +- Filter ((size(features#360, true) > 0) AND isnotnull(features#360))\n",
      "                                    +- FileScan geojson [features#360] Batched: false, DataFilters: [(size(features#360, true) > 0), isnotnull(features#360)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "# We initialized a spark session with specific configurations, now we import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, regexp_replace, to_timestamp, year, lit, count, sum, corr, desc, asc\n",
    "from sedona.spark import *\n",
    "import time\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "#Beginning of timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Import census blocks dataset (geojson)\n",
    "census_raw_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\") \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "census_df = census_raw_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                census_raw_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "census_df = census_df.select(\n",
    "    col(\"POP20\").cast(DoubleType()).alias(\"population_2020\"),\n",
    "    col(\"ZCTA20\").alias(\"zip_code_key\"), \n",
    "    col(\"geometry\").alias(\"census_geom\"),\n",
    "    col(\"HOUSING20\").alias(\"housing_2020\"),\n",
    "    \"COMM\"\n",
    ").filter(col(\"zip_code_key\").isNotNull() & (col(\"population_2020\") > 0))\n",
    "\n",
    "# For income data\n",
    "income_schema = StructType([\n",
    "    StructField(\"Zip Code\", StringType(), True),\n",
    "    StructField(\"Community\", StringType(), True),\n",
    "    StructField(\"Estimated Median Income\", StringType(), True)\n",
    "])\n",
    "\n",
    "income_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\", \\\n",
    "    header=True, \n",
    "    sep=\";\",              \n",
    "    schema=income_schema) \n",
    "\n",
    "income_df = income_df.select(\n",
    "    col(\"Zip Code\").cast(StringType()).alias(\"zip_code_key\"), \n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(DoubleType()).alias(\"income_2021\") \n",
    ")\n",
    "\n",
    "# Crimes dataset\n",
    "crime_data_full_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Create DataFrame from main dataset\n",
    "crime_data_full_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/\", \\\n",
    "    header=True, \\\n",
    "    schema=crime_data_full_schema) \n",
    "\n",
    "# Filter out nulls and Null Island, shorten the Dataframe and add coordinate points\n",
    "crime_geo_df = crime_data_full_df.select(\"DR_NO\", \"Date Rptd\", \"LAT\", \"LON\") \\\n",
    "    .filter(col(\"LAT\").isNotNull() & col(\"LON\").isNotNull()) \\\n",
    "    .filter(~((col(\"LAT\") == 0) & (col(\"LON\") == 0))) \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# Find the year\n",
    "crime_geo_df = crime_geo_df.withColumn(\n",
    "    'Timestamp', to_timestamp(col('Date Rptd'), 'yyyy MMM dd hh:mm:ss a') ) \\\n",
    "    .withColumn('year', year(col('Timestamp')))\n",
    "\n",
    "crime_geo_df = crime_geo_df.select(\"DR_NO\", \"year\", \"crime_geom\") \\\n",
    "    .filter(col(\"year\").isNotNull()) \\\n",
    "    .filter((col(\"year\") == lit(2020)) | (col(\"year\") == lit(2021)))\n",
    "\n",
    "# House income per block\n",
    "census_income_df = census_df.join(income_df, on=\"zip_code_key\", how=\"inner\") \\\n",
    "    .withColumn(\"income_per_block\",col(\"income_2021\") * col(\"housing_2020\").cast(DoubleType()))\n",
    "\n",
    "# Income per person for each community\n",
    "census_income_df = census_income_df.groupBy(\"COMM\").agg(sum(col(\"population_2020\")).alias(\"population\"), sum(col(\"income_per_block\")).alias(\"sum_income\"))\n",
    "\n",
    "census_income_df = census_income_df.withColumn(\"avg income/person\", col(\"sum_income\") / col(\"population\")) \\\n",
    "    .drop(\"sum_income\")\n",
    "\n",
    "# Crimes per Community\n",
    "zip_crimes_df = crime_geo_df.join(census_df, ST_Contains(col(\"census_geom\"), col(\"crime_geom\")), \"inner\") \\\n",
    " .groupBy(\"COMM\",).agg(count(\"DR_NO\").alias(\"crime_count\"))\n",
    "\n",
    "# Final result\n",
    "final_df = census_income_df.join(\n",
    "    zip_crimes_df, \n",
    "    on=\"COMM\", \n",
    "    how=\"inner\") \\\n",
    "    .withColumn(\"avg crime/person\", col(\"crime_count\")/ (2*col(\"population\")) )\\\n",
    "    .drop(\"population\", \"crime_count\")\n",
    "\n",
    "# Show results & correlation\n",
    "final_df.collect()\n",
    "print(\"\\n--- Correlation ---\")\n",
    "final_df.select(\n",
    "    corr(\"avg crime/person\", \"avg income/person\").alias(\"Correlation\")\n",
    ").show()\n",
    "\n",
    "# Top 10\n",
    "print(\"Correlation Top 10:\")\n",
    "final_df.orderBy(desc(\"avg income/person\")).limit(10) \\\n",
    "    .select(corr(\"avg crime/person\", \"avg income/person\").alias(\"Correlation_Top_10\")).show()\n",
    "\n",
    "# Bottom 10\n",
    "print(\"Correlation Bottom 10:\")\n",
    "final_df.orderBy(asc(\"avg income/person\")).limit(10) \\\n",
    "    .select(corr(\"avg crime/person\", \"avg income/person\").alias(\"Correlation_Bottom_10\")).show()\n",
    "\n",
    "# End of timing\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\nExecution time: {execution_time} seconds\")\n",
    "\n",
    "# Show plan\n",
    "final_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd8291-4f43-4529-be3c-165782a5f32e",
   "metadata": {},
   "source": [
    "## Other Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3349d-ca37-4168-b207-f3cb1416c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cee3d-a913-4ea7-a94a-e103e5132d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
