{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8e88b4-7b4b-42c0-aef8-ad8c66d9d906",
   "metadata": {},
   "source": [
    "# Query 4\n",
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061057fc-1f22-4541-9f6a-729bd0b0d4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1805</td><td>application_1765289937462_1789</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1789/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1789_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1807</td><td>application_1765289937462_1791</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3147971c-2cd8-4764-b815-712395e3f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1808</td><td>application_1765289937462_1792</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1792/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1792_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c89f0c6d9a54ad4bf1492d734132948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f2c2150b1843049739467a3f21e85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------+\n",
      "|        division|average_distance|     #|\n",
      "+----------------+----------------+------+\n",
      "|       HOLLYWOOD|           2.074|224124|\n",
      "|        VAN NUYS|           2.939|208129|\n",
      "|       SOUTHWEST|           2.191|189119|\n",
      "|        WILSHIRE|           2.593|186383|\n",
      "|     77TH STREET|           1.717|170620|\n",
      "| NORTH HOLLYWOOD|           2.642|168096|\n",
      "|         OLYMPIC|           1.729|162805|\n",
      "|         PACIFIC|           3.853|162027|\n",
      "|         CENTRAL|           0.993|154689|\n",
      "|         RAMPART|           1.534|153204|\n",
      "|       SOUTHEAST|           2.444|143803|\n",
      "|     WEST VALLEY|           3.022|136622|\n",
      "|        FOOTHILL|            4.26|132482|\n",
      "|         TOPANGA|           3.297|131054|\n",
      "|          HARBOR|           3.702|127071|\n",
      "|      HOLLENBECK|           2.677|116235|\n",
      "|WEST LOS ANGELES|            2.79|115969|\n",
      "|          NEWTON|           1.635|111392|\n",
      "|       NORTHEAST|           3.623|108243|\n",
      "|         MISSION|           3.676| 97926|\n",
      "|      DEVONSHIRE|           2.825| 77180|\n",
      "+----------------+----------------+------+\n",
      "\n",
      "\n",
      "Execution time: 50.65235900878906 seconds\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##147L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##147L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=485]\n",
      "      +- HashAggregate(keys=[division#101], functions=[avg(distance#121), count(DR_NO#24)], schema specialized)\n",
      "         +- Exchange hashpartitioning(division#101, 1000), ENSURE_REQUIREMENTS, [plan_id=482]\n",
      "            +- HashAggregate(keys=[division#101], functions=[partial_avg(distance#121), partial_count(DR_NO#24)], schema specialized)\n",
      "               +- Project [DR_NO#24, division#101, distance#121]\n",
      "                  +- Filter (min_rank#135 = 1)\n",
      "                     +- Window [row_number() windowspecdefinition(DR_NO#24, distance#121 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS min_rank#135], [DR_NO#24], [distance#121 ASC NULLS FIRST]\n",
      "                        +- WindowGroupLimit [DR_NO#24], [distance#121 ASC NULLS FIRST], row_number(), 1, Final\n",
      "                           +- Sort [DR_NO#24 ASC NULLS FIRST, distance#121 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(DR_NO#24, 1000), ENSURE_REQUIREMENTS, [plan_id=474]\n",
      "                                 +- WindowGroupLimit [DR_NO#24], [distance#121 ASC NULLS FIRST], row_number(), 1, Partial\n",
      "                                    +- Sort [DR_NO#24 ASC NULLS FIRST, distance#121 ASC NULLS FIRST], false, 0\n",
      "                                       +- Project [DR_NO#24, division#101, ( **org.apache.spark.sql.sedona_sql.expressions.ST_DistanceSphere**   / 1000.0) AS distance#121]\n",
      "                                          +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "                                             :- Project [DR_NO#24,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#84]\n",
      "                                             :  +- Filter ((isnotnull(LAT#50) AND isnotnull(LON#51)) AND (NOT (LAT#50 = 0.0) OR NOT (LON#51 = 0.0)))\n",
      "                                             :     +- FileScan csv [DR_NO#24,LAT#50,LON#51] Batched: false, DataFilters: [isnotnull(LAT#50), isnotnull(LON#51), (NOT (LAT#50 = 0.0) OR NOT (LON#51 = 0.0))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,LAT:double,LON:double>\n",
      "                                             +- BroadcastExchange IdentityBroadcastMode, [plan_id=467]\n",
      "                                                +- Project [DIVISION#92 AS division#101,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#108]\n",
      "                                                   +- FileScan csv [X#89,Y#90,DIVISION#92] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_P..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:double,Y:double,DIVISION:string>"
     ]
    }
   ],
   "source": [
    "# We initialized a spark session with specific configurations, now we import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, row_number, avg, round, count\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "#Beginning of timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Define schema for crime data DataFrame\n",
    "crime_data_full_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Create DataFrame from main dataset\n",
    "crime_data_full_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/\", \\\n",
    "    header=True, \\\n",
    "    schema=crime_data_full_schema) \n",
    "\n",
    "# Filter out nulls and Null Island, shorten the Dataframe and add coordinate points\n",
    "crime_geo_df = crime_data_full_df.select(\"DR_NO\", \"LAT\", \"LON\") \\\n",
    "    .filter(col(\"LAT\").isNotNull() & col(\"LON\").isNotNull()) \\\n",
    "    .filter(~((col(\"LAT\") == 0) & (col(\"LON\") == 0))) \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# Schema and DataFrame of police stations dataset\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", DoubleType()),       # Longitude\n",
    "    StructField(\"Y\", DoubleType()),       # Latitude\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType()),\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\", \n",
    "    header=True, \n",
    "    schema=stations_schema) \\\n",
    "    .select(col(\"DIVISION\").alias(\"division\"), \n",
    "            col(\"Y\").alias(\"station_lat\"),  \n",
    "            col(\"X\").alias(\"station_lon\")) \\\n",
    "    .withColumn(\"station_geom\", ST_Point(\"station_lon\", \"station_lat\"))\n",
    "\n",
    "# Join the 2 dataframes\n",
    "cross_joined_df = crime_geo_df.crossJoin(stations_df)\n",
    "\n",
    "# Calculate the distance between crimes and police stations\n",
    "distance_df = cross_joined_df.withColumn(\"distance\", ST_DistanceSphere(\"crime_geom\", \"station_geom\")/1000) \\\n",
    "    .select(\"DR_NO\", \"division\", \"distance\")\n",
    "\n",
    "# Find the closest station to each crime\n",
    "window_crime = Window.partitionBy(\"DR_NO\").orderBy(col(\"distance\").asc())\n",
    "\n",
    "closest_station_df = distance_df.withColumn(\"min_rank\", row_number().over(window_crime)\n",
    ").filter(col(\"min_rank\") == 1)\n",
    "\n",
    "# Find average distance and # of closest crimes for each station \n",
    "final_result_df = closest_station_df.groupBy(\"division\").agg(\n",
    "    round(avg(col(\"distance\")), 3).alias(\"average_distance\"),\n",
    "    count(col(\"DR_NO\")).alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "# Show results\n",
    "final_result_df.show(21)\n",
    "\n",
    "# End of timing\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\nExecution time: {execution_time} seconds\")\n",
    "\n",
    "# Show plan\n",
    "final_result_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e674bc1-75f7-4ef9-83f9-21ee8415ea24",
   "metadata": {},
   "source": [
    "## Other configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b96646-1c2b-4fca-94b9-94c75f037090",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41651e68-2496-42c0-8554-0725d4ab4417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1699</td><td>application_1765289937462_1683</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1683/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1683_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e86ba377304afaa33331924d48ef7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1693</td><td>application_1765289937462_1677</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1677/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1677_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1695</td><td>application_1765289937462_1679</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1679/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1679_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1699</td><td>application_1765289937462_1683</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1683/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1683_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr><tr><td>1701</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b9250-baee-46b7-b1f7-a6257d63b892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
