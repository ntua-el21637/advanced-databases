{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232f744b-3bac-41a5-8c70-1c44ad01707b",
   "metadata": {},
   "source": [
    "# Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c5f1ea-df7a-4300-84b1-a5b2b8f7527f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.cores': '1', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1817</td><td>application_1765289937462_1801</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1801/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-213.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1801_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1820</td><td>application_1765289937462_1804</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1804/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-251.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1804_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1821</td><td>application_1765289937462_1805</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1805/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1805_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1822</td><td>application_1765289937462_1806</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1806/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1806_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.executor.instances\": \"4\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.memory\": \"2g\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444bddb-fb1e-4afd-b51d-ff7587c44b34",
   "metadata": {},
   "source": [
    "## DataFrame API υλοποίηση"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a0b929-2fbe-4874-a4e8-f427e9679498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bca6766a622462ebb27397119411405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "DF Join BROADCAST\n",
      "==============================\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [mo_code])\n",
      ":- Project [trim(cast(mo_code#846 as string), None) AS mo_code#866, count#852L]\n",
      ":  +- Aggregate [mo_code#846], [mo_code#846, count(1) AS count#852L]\n",
      ":     +- Filter (isnotnull(mo_code#846) AND NOT (mo_code#846 = ))\n",
      ":        +- Project [Mocodes#797, mo_code#846]\n",
      ":           +- Generate explode(split(Mocodes#797, \\s+, -1)), false, [mo_code#846]\n",
      ":              +- Filter isnotnull(Mocodes#797)\n",
      ":                 +- Project [Mocodes#797]\n",
      ":                    +- Relation [DR_NO#787,Date Rptd#788,DATE OCC#789,TIME OCC#790,AREA#791,AREA NAME#792,Rpt Dist No#793,Part 1-2#794,Crm Cd#795,Crm Cd Desc#796,Mocodes#797,Vict Age#798,Vict Sex#799,Vict Descent#800,Premis Cd#801,Premis Desc#802,Weapon Used Cd#803,Weapon Desc#804,Status#805,Status Desc#806,Crm Cd 1#807,Crm Cd 2#808,Crm Cd 3#809,Crm Cd 4#810,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Project [trim(cast(mo_code#855 as string), None) AS mo_code#869, description#856]\n",
      "      +- LogicalRDD [mo_code#855, description#856], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mo_code: string, count: bigint, description: string\n",
      "Project [mo_code#866, count#852L, description#856]\n",
      "+- Join LeftOuter, (mo_code#866 = mo_code#869)\n",
      "   :- Project [trim(cast(mo_code#846 as string), None) AS mo_code#866, count#852L]\n",
      "   :  +- Aggregate [mo_code#846], [mo_code#846, count(1) AS count#852L]\n",
      "   :     +- Filter (isnotnull(mo_code#846) AND NOT (mo_code#846 = ))\n",
      "   :        +- Project [Mocodes#797, mo_code#846]\n",
      "   :           +- Generate explode(split(Mocodes#797, \\s+, -1)), false, [mo_code#846]\n",
      "   :              +- Filter isnotnull(Mocodes#797)\n",
      "   :                 +- Project [Mocodes#797]\n",
      "   :                    +- Relation [DR_NO#787,Date Rptd#788,DATE OCC#789,TIME OCC#790,AREA#791,AREA NAME#792,Rpt Dist No#793,Part 1-2#794,Crm Cd#795,Crm Cd Desc#796,Mocodes#797,Vict Age#798,Vict Sex#799,Vict Descent#800,Premis Cd#801,Premis Desc#802,Weapon Used Cd#803,Weapon Desc#804,Status#805,Status Desc#806,Crm Cd 1#807,Crm Cd 2#808,Crm Cd 3#809,Crm Cd 4#810,... 4 more fields] csv\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- Project [trim(cast(mo_code#855 as string), None) AS mo_code#869, description#856]\n",
      "         +- LogicalRDD [mo_code#855, description#856], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [mo_code#866, count#852L, description#856]\n",
      "+- Join LeftOuter, (mo_code#866 = mo_code#869), rightHint=(strategy=broadcast)\n",
      "   :- InMemoryRelation [mo_code#866, count#852L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :     +- AdaptiveSparkPlan isFinalPlan=true\n",
      "            +- == Final Plan ==\n",
      "               *(2) HashAggregate(keys=[mo_code#88], functions=[count(1)], output=[mo_code#108, count#94L], schema specialized)\n",
      "               +- ShuffleQueryStage 0\n",
      "                  +- Exchange hashpartitioning(mo_code#88, 1000), ENSURE_REQUIREMENTS, [plan_id=152]\n",
      "                     +- *(1) HashAggregate(keys=[mo_code#88], functions=[partial_count(1)], output=[mo_code#88, count#120L], schema specialized)\n",
      "                        +- *(1) Filter NOT (mo_code#88 = )\n",
      "                           +- *(1) Generate explode(split(Mocodes#39, \\s+, -1)), false, [mo_code#88]\n",
      "                              +- *(1) Filter isnotnull(Mocodes#39)\n",
      "                                 +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "            +- == Initial Plan ==\n",
      "               HashAggregate(keys=[mo_code#88], functions=[count(1)], output=[mo_code#108, count#94L], schema specialized)\n",
      "               +- Exchange hashpartitioning(mo_code#88, 1000), ENSURE_REQUIREMENTS, [plan_id=107]\n",
      "                  +- HashAggregate(keys=[mo_code#88], functions=[partial_count(1)], output=[mo_code#88, count#120L], schema specialized)\n",
      "                     +- Filter NOT (mo_code#88 = )\n",
      "                        +- Generate explode(split(Mocodes#39, \\s+, -1)), false, [mo_code#88]\n",
      "                           +- Filter isnotnull(Mocodes#39)\n",
      "                              +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "   +- Project [trim(mo_code#855, None) AS mo_code#869, description#856]\n",
      "      +- Filter isnotnull(trim(mo_code#855, None))\n",
      "         +- LogicalRDD [mo_code#855, description#856], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [mo_code#866, count#852L, description#856]\n",
      "   +- BroadcastHashJoin [mo_code#866], [mo_code#869], LeftOuter, BuildRight, false\n",
      "      :- InMemoryTableScan [mo_code#866, count#852L]\n",
      "      :     +- InMemoryRelation [mo_code#866, count#852L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :           +- AdaptiveSparkPlan isFinalPlan=true\n",
      "                     +- == Final Plan ==\n",
      "                        *(2) HashAggregate(keys=[mo_code#88], functions=[count(1)], output=[mo_code#108, count#94L], schema specialized)\n",
      "                        +- ShuffleQueryStage 0\n",
      "                           +- Exchange hashpartitioning(mo_code#88, 1000), ENSURE_REQUIREMENTS, [plan_id=152]\n",
      "                              +- *(1) HashAggregate(keys=[mo_code#88], functions=[partial_count(1)], output=[mo_code#88, count#120L], schema specialized)\n",
      "                                 +- *(1) Filter NOT (mo_code#88 = )\n",
      "                                    +- *(1) Generate explode(split(Mocodes#39, \\s+, -1)), false, [mo_code#88]\n",
      "                                       +- *(1) Filter isnotnull(Mocodes#39)\n",
      "                                          +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "                     +- == Initial Plan ==\n",
      "                        HashAggregate(keys=[mo_code#88], functions=[count(1)], output=[mo_code#108, count#94L], schema specialized)\n",
      "                        +- Exchange hashpartitioning(mo_code#88, 1000), ENSURE_REQUIREMENTS, [plan_id=107]\n",
      "                           +- HashAggregate(keys=[mo_code#88], functions=[partial_count(1)], output=[mo_code#88, count#120L], schema specialized)\n",
      "                              +- Filter NOT (mo_code#88 = )\n",
      "                                 +- Generate explode(split(Mocodes#39, \\s+, -1)), false, [mo_code#88]\n",
      "                                    +- Filter isnotnull(Mocodes#39)\n",
      "                                       +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1553]\n",
      "         +- Project [trim(mo_code#855, None) AS mo_code#869, description#856]\n",
      "            +- Filter isnotnull(trim(mo_code#855, None))\n",
      "               +- Scan ExistingRDD[mo_code#855,description#856]\n",
      "\n",
      "DF Join BROADCAST -> 1.600 sec | rows: 774\n",
      "FINAL DF top20 -> 1.390 sec | rows fetched: 20\n",
      "\n",
      "==============================\n",
      "FINAL DF top20 -> 1.390 sec | rows fetched: 20\n",
      "==============================\n",
      "+-------+--------------------------------------------------------------------------------+-------+\n",
      "|mo_code|description                                                                     |count  |\n",
      "+-------+--------------------------------------------------------------------------------+-------+\n",
      "|0344   |Removes vict property                                                           |1002900|\n",
      "|1822   |Stranger                                                                        |548422 |\n",
      "|0416   |Hit-Hit w/ weapon                                                               |404773 |\n",
      "|0329   |Vandalized                                                                      |377536 |\n",
      "|0913   |Victim knew Suspect                                                             |278618 |\n",
      "|2000   |Domestic violence                                                               |256188 |\n",
      "|1300   |Vehicle involved                                                                |219082 |\n",
      "|0400   |Force used                                                                      |213165 |\n",
      "|1402   |Evidence Booked (any crime)                                                     |177470 |\n",
      "|1609   |Smashed                                                                         |131229 |\n",
      "|1309   |Susp uses vehicle                                                               |122108 |\n",
      "|1202   |Victim was aged (60 & over) or blind/physically disabled/unable to care for self|120238 |\n",
      "|0325   |Took merchandise                                                                |120159 |\n",
      "|1814   |Susp is/was current/former boyfriend/girlfriend                                 |118073 |\n",
      "|0444   |Pushed                                                                          |116763 |\n",
      "|1501   |Other MO (see rpt)                                                              |115589 |\n",
      "|1307   |Breaks window                                                                   |113609 |\n",
      "|0334   |Brandishes weapon                                                               |105665 |\n",
      "|2004   |Suspect is homeless/transient                                                   |93426  |\n",
      "|0432   |Intimidation                                                                    |83562  |\n",
      "+-------+--------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Join timing summary (sec):\n",
      "broadcast: 1.6000163555145264"
     ]
    }
   ],
   "source": [
    "import time, csv\n",
    "from io import StringIO\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# =====================================================\n",
    "# Query 3 — MO Codes frequency + descriptions mapping\n",
    "# DF + RDD + join strategies (hint/explain) + timings\n",
    "# =====================================================\n",
    "\n",
    "CRIME_DIR = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/\"\n",
    "MO_PATH   = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: robust MO lookup loader\n",
    "# ---------------------------\n",
    "def load_mo_lookup(mo_path: str):\n",
    "    \"\"\"\n",
    "    MO_codes.txt format (per assignment): each line starts with code, then a space, then description.\n",
    "    Return DF: (mo_code, description)\n",
    "    \"\"\"\n",
    "    raw = sc.textFile(mo_path)\n",
    "\n",
    "    # keep only lines that start with a digit (avoid empty/header junk)\n",
    "    raw2 = raw.filter(lambda line: line and line.strip() and line.strip()[0].isdigit())\n",
    "\n",
    "    def parse_line(line):\n",
    "        # split first token (code) and rest (description)\n",
    "        parts = line.strip().split(None, 1)\n",
    "        code = parts[0].strip()\n",
    "        desc = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        return (code, desc)\n",
    "\n",
    "    mo_rdd = raw2.map(parse_line).filter(lambda x: x[0] != \"\")\n",
    "    mo_df = spark.createDataFrame(mo_rdd, [\"mo_code\", \"description\"])\n",
    "    mo_df.limit(1).count()\n",
    "    return mo_df\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: timing + explain\n",
    "# ---------------------------\n",
    "def time_and_explain(df, label):\n",
    "    print(\"\\n==============================\")\n",
    "    print(label)\n",
    "    print(\"==============================\")\n",
    "    df.explain(True)\n",
    "    _ = df.limit(1).count()  # warm-up\n",
    "    t0 = time.time()\n",
    "    n = df.count()\n",
    "    dt = time.time() - t0\n",
    "    print(f\"{label} -> {dt:.3f} sec | rows: {n}\")\n",
    "    return dt\n",
    "\n",
    "# =====================================================\n",
    "# =============== DataFrame API =======================\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "crime_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(CRIME_DIR)\n",
    ")\n",
    "\n",
    "crime_mo_exploded = (\n",
    "    crime_df\n",
    "    .select(\"Mocodes\")\n",
    "    .filter(F.col(\"Mocodes\").isNotNull())\n",
    "    .withColumn(\"mo_code\", F.explode(F.split(F.col(\"Mocodes\"), r\"\\s+\")))\n",
    "    .filter((F.col(\"mo_code\").isNotNull()) & (F.col(\"mo_code\") != \"\"))\n",
    ")\n",
    "\n",
    "mo_counts = crime_mo_exploded.groupBy(\"mo_code\").count()\n",
    "\n",
    "mo_codes = load_mo_lookup(MO_PATH)\n",
    "\n",
    "mo_counts_norm = mo_counts.withColumn(\"mo_code\", F.trim(F.col(\"mo_code\").cast(\"string\")))\n",
    "mo_codes_norm  = mo_codes.withColumn(\"mo_code\", F.trim(F.col(\"mo_code\").cast(\"string\")))\n",
    "\n",
    "# CACHE το ακριβό κομμάτι (explode+groupBy) για να μην ξανατρέχει σε κάθε join\n",
    "mo_counts_norm = mo_counts_norm.cache()\n",
    "mo_counts_norm.count()  # materialize once (ζεσταίνει + χτίζει το cache)\n",
    "\n",
    "# Join strategies\n",
    "#df_baseline = mo_counts_norm.join(mo_codes_norm, on=\"mo_code\", how=\"left\")\n",
    "#t_baseline  = time_and_explain(df_baseline, \"DF Join baseline (no hint)\")\n",
    "\n",
    "df_bcast    = mo_counts_norm.join(broadcast(mo_codes_norm), on=\"mo_code\", how=\"left\")\n",
    "t_bcast     = time_and_explain(df_bcast, \"DF Join BROADCAST\")\n",
    "\n",
    "#df_merge    = mo_counts_norm.hint(\"MERGE\").join(mo_codes_norm.hint(\"MERGE\"), on=\"mo_code\", how=\"left\")\n",
    "#t_merge     = time_and_explain(df_merge, \"DF Join MERGE\")\n",
    "\n",
    "#spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "#spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")\n",
    "\n",
    "#df_hash     = mo_counts_norm.hint(\"SHUFFLE_HASH\").join(mo_codes_norm.hint(\"SHUFFLE_HASH\"), on=\"mo_code\", how=\"left\")\n",
    "#t_hash      = time_and_explain(df_hash, \"DF Join SHUFFLE_HASH\")\n",
    "\n",
    "#df_nl       = mo_counts_norm.hint(\"SHUFFLE_REPLICATE_NL\").join(\n",
    "#                mo_codes_norm.hint(\"SHUFFLE_REPLICATE_NL\"),\n",
    "#                on=\"mo_code\",\n",
    "#                how=\"left\"\n",
    "#             )\n",
    "#t_nl        = time_and_explain(df_nl, \"DF Join SHUFFLE_REPLICATE_NL\")\n",
    "\n",
    "#spark.conf.unset(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "#spark.conf.unset(\"spark.sql.join.preferSortMergeJoin\")\n",
    "\n",
    "final_df = (df_bcast\n",
    "            .orderBy(F.col(\"count\").desc())\n",
    "            .select(\"mo_code\", \"description\", \"count\"))\n",
    "\n",
    "_ = final_df.limit(1).count()  # warm-up\n",
    "t0 = time.time()\n",
    "top20_rows = final_df.limit(20).collect()\n",
    "df_total = time.time() - t0\n",
    "print(f\"FINAL DF top20 -> {df_total:.3f} sec | rows fetched: {len(top20_rows)}\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"FINAL DF top20 -> {df_total:.3f} sec | rows fetched: {len(top20_rows)}\")\n",
    "print(\"==============================\")\n",
    "final_df.show(20, truncate=False)\n",
    "\n",
    "print(\"\\nJoin timing summary (sec):\")\n",
    "#print(\"baseline:\", t_baseline)\n",
    "print(\"broadcast:\", t_bcast)\n",
    "#print(\"merge:\", t_merge)\n",
    "#print(\"shuffle_hash:\", t_hash)\n",
    "#print(\"shuffle_replicate_nl:\", t_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a230dcb5-b949-4845-a610-bc828c6a84c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463d23ad2c624b18b8001bd5c21342ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RDD pipeline (Corrected to include Description Join) ===\n",
      "\n",
      "==============================\n",
      "RDD time -> 29.737 sec (top20 computed)\n",
      "==============================\n",
      "Top 20 MO codes (RDD) with Description:\n",
      "Count      Code       Description\n",
      "--------------------------------------------------\n",
      "1002900    0344       Removes vict property\n",
      "548422     1822       Stranger\n",
      "404773     0416       Hit-Hit w/ weapon\n",
      "377536     0329       Vandalized\n",
      "278618     0913       Victim knew Suspect\n",
      "256188     2000       Domestic violence\n",
      "219082     1300       Vehicle involved\n",
      "213165     0400       Force used\n",
      "177470     1402       Evidence Booked (any crime)\n",
      "131229     1609       Smashed\n",
      "122108     1309       Susp uses vehicle\n",
      "120238     1202       Victim was aged (60 & over) or blind/physically disabled/unable to care for self\n",
      "120159     0325       Took merchandise\n",
      "118073     1814       Susp is/was current/former boyfriend/girlfriend\n",
      "116763     0444       Pushed\n",
      "115589     1501       Other MO (see rpt)\n",
      "113609     1307       Breaks window\n",
      "105665     0334       Brandishes weapon\n",
      "93426      2004       Suspect is homeless/transient\n",
      "83562      0432       Intimidation"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# ================== RDD API ==========================\n",
    "# =====================================================\n",
    "import time, csv\n",
    "from io import StringIO\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# =====================================================\n",
    "# Query 3 — MO Codes frequency + descriptions mapping\n",
    "# DF + RDD + join strategies (hint/explain) + timings\n",
    "# =====================================================\n",
    "\n",
    "CRIME_DIR = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/\"\n",
    "MO_PATH   = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: robust MO lookup loader\n",
    "# ---------------------------\n",
    "def load_mo_lookup(mo_path: str):\n",
    "    \"\"\"\n",
    "    MO_codes.txt format (per assignment): each line starts with code, then a space, then description.\n",
    "    Return DF: (mo_code, description)\n",
    "    \"\"\"\n",
    "    raw = sc.textFile(mo_path)\n",
    "\n",
    "    # keep only lines that start with a digit (avoid empty/header junk)\n",
    "    raw2 = raw.filter(lambda line: line and line.strip() and line.strip()[0].isdigit())\n",
    "\n",
    "    def parse_line(line):\n",
    "        # split first token (code) and rest (description)\n",
    "        parts = line.strip().split(None, 1)\n",
    "        code = parts[0].strip()\n",
    "        desc = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        return (code, desc)\n",
    "\n",
    "    mo_rdd = raw2.map(parse_line).filter(lambda x: x[0] != \"\")\n",
    "    mo_df = spark.createDataFrame(mo_rdd, [\"mo_code\", \"description\"])\n",
    "    mo_df.limit(1).count()\n",
    "    return mo_df\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: timing + explain\n",
    "# ---------------------------\n",
    "def time_and_explain(df, label):\n",
    "    print(\"\\n==============================\")\n",
    "    print(label)\n",
    "    print(\"==============================\")\n",
    "    df.explain(True)\n",
    "    _ = df.limit(1).count()  # warm-up\n",
    "    t0 = time.time()\n",
    "    n = df.count()\n",
    "    dt = time.time() - t0\n",
    "    print(f\"{label} -> {dt:.3f} sec | rows: {n}\")\n",
    "    return dt\n",
    "print(\"\\n=== RDD pipeline (Corrected to include Description Join) ===\")\n",
    "\n",
    "start_rdd = time.time()\n",
    "\n",
    "raw = sc.textFile(CRIME_DIR)\n",
    "header = raw.first()\n",
    "\n",
    "def split_row(line):\n",
    "    return next(csv.reader(StringIO(line)))\n",
    "\n",
    "rdd = raw.filter(lambda x: x != header).map(split_row)\n",
    "\n",
    "cols = [c.strip('\"') for c in header.split(\",\")]\n",
    "mo_idx = cols.index(\"Mocodes\")\n",
    "\n",
    "# RDD με ζεύγη (mo_code, count)\n",
    "mo_rdd_counts = (rdd\n",
    "    .filter(lambda r: r[mo_idx] is not None and r[mo_idx] != \"\")\n",
    "    .flatMap(lambda r: r[mo_idx].split(\" \"))\n",
    "    .filter(lambda x: x is not None and x != \"\")\n",
    "    .map(lambda mo: (mo.strip(), 1)) # Key = mo_code (string)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "# 2. RDD για MO Codes Lookup \n",
    "raw_lookup = sc.textFile(MO_PATH)\n",
    "raw2_lookup = raw_lookup.filter(lambda line: line and line.strip() and line.strip()[0].isdigit())\n",
    "\n",
    "def parse_line_rdd(line):\n",
    "    parts = line.strip().split(None, 1)\n",
    "    code = parts[0].strip()\n",
    "    desc = parts[1].strip() if len(parts) > 1 else \"Unknown Description\" # Default value\n",
    "    return (code, desc)\n",
    "\n",
    "# RDD με ζεύγη (mo_code, description)\n",
    "mo_rdd_lookup = raw2_lookup.map(parse_line_rdd).filter(lambda x: x[0] != \"\")\n",
    "\n",
    "# JOIN RDDs (Pair RDD Join), Result: (mo_code, (count, description))\n",
    "joined_rdd = mo_rdd_counts.leftOuterJoin(mo_rdd_lookup) \n",
    "\n",
    "# Order and: (mo_code, (count, (description ή None))) --> (count, mo_code, description)\n",
    "final_rdd_sorted = (joined_rdd\n",
    "    .map(lambda x: (\n",
    "        x[1][0], \n",
    "        x[0], \n",
    "        x[1][1] if x[1][1] is not None else \"No Description Found\"\n",
    "    ))\n",
    "    .sortBy(lambda x: x[0], ascending=False)\n",
    ")\n",
    "\n",
    "top20 = final_rdd_sorted.take(20)\n",
    "rdd_time = time.time() - start_rdd\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"RDD time -> {rdd_time:.3f} sec (top20 computed)\")\n",
    "print(\"==============================\")\n",
    "print(\"Top 20 MO codes (RDD) with Description:\")\n",
    "print(\"{:<10} {:<10} {}\".format(\"Count\", \"Code\", \"Description\"))\n",
    "print(\"-\" * 50)\n",
    "for cnt, code, desc in top20:\n",
    "    print(\"{:<10} {:<10} {}\".format(cnt, code, desc))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
